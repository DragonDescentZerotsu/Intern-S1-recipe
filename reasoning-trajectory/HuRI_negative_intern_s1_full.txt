/data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
INFO 11-22 17:03:35 [__init__.py:216] Automatically detected platform cuda.
2025-11-22 17:03:41,934 - INFO - Logging initialized. Log file: /data1/tianang/Projects/Intern-S1/logs/experiment_log_20251122_170341.log
2025-11-22 17:03:41,934 - INFO - Selected task groups: ['PPI']
2025-11-22 17:03:41,934 - INFO - Sampling times: 16
INFO 11-22 17:03:42 [utils.py:233] non-default args: {'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 24576, 'tensor_parallel_size': 8, 'gpu_memory_utilization': 0.92, 'max_num_batched_tokens': 24576, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'limit_mm_per_prompt': {'video': 0, 'image': 0}, 'model': 'internlm/Intern-S1'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 11-22 17:03:44 [model.py:547] Resolved architecture: InternS1ForConditionalGeneration
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 11-22 17:03:44 [model.py:1510] Using max model len 24576
INFO 11-22 17:03:44 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=24576.
INFO 11-22 17:03:44 [__init__.py:381] Cudagraph is disabled under eager mode
WARNING 11-22 17:03:45 [tokenizer.py:253] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 11-22 17:03:49 [tokenizer.py:253] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 11-22 17:03:49 [registry.py:117] All limits of multimodal modalities supported by the model are set to 0, running in text-only mode.
/data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
INFO 11-22 17:03:57 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:04:03 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:04:03 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='internlm/Intern-S1', speculative_config=None, tokenizer='internlm/Intern-S1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=24576, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=internlm/Intern-S1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=3809986)[0;0m WARNING 11-22 17:04:03 [multiproc_executor.py:720] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:04:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_0f9dbbe9'), local_subscribe_addr='ipc:///local/tmp/afb28256-7703-40f1-a547-d3fdcb6a63b9', remote_subscribe_addr=None, remote_addr_ipv6=False)
/data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
INFO 11-22 17:04:11 [__init__.py:216] Automatically detected platform cuda.
INFO 11-22 17:04:11 [__init__.py:216] Automatically detected platform cuda.
INFO 11-22 17:04:11 [__init__.py:216] Automatically detected platform cuda.
INFO 11-22 17:04:11 [__init__.py:216] Automatically detected platform cuda.
INFO 11-22 17:04:12 [__init__.py:216] Automatically detected platform cuda.
INFO 11-22 17:04:12 [__init__.py:216] Automatically detected platform cuda.
INFO 11-22 17:04:12 [__init__.py:216] Automatically detected platform cuda.
INFO 11-22 17:04:18 [__init__.py:216] Automatically detected platform cuda.
INFO 11-22 17:04:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_27b06fb3'), local_subscribe_addr='ipc:///local/tmp/fe57529a-f7a3-4105-9653-c08446de59c7', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-22 17:04:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7530ee38'), local_subscribe_addr='ipc:///local/tmp/79d8e96b-1a84-46f8-84d5-b7a3728361e3', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-22 17:04:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8076107f'), local_subscribe_addr='ipc:///local/tmp/a6948103-0fe8-4c73-912c-8d548afcfdd6', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-22 17:04:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3bb7afec'), local_subscribe_addr='ipc:///local/tmp/30f8e77d-462f-41b4-8bb5-034f525a4a39', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-22 17:04:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_14531f81'), local_subscribe_addr='ipc:///local/tmp/749d2a8c-7aa2-4438-9f9e-5768574d8201', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-22 17:04:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0dc086b7'), local_subscribe_addr='ipc:///local/tmp/e991d73f-8a99-4d9a-8f8e-92c48aa36524', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 11-22 17:04:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3516a9bc'), local_subscribe_addr='ipc:///local/tmp/6b26747f-417c-4be7-b45c-a372fceedd34', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 11-22 17:04:21 [tokenizer.py:253] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 11-22 17:04:21 [registry.py:117] All limits of multimodal modalities supported by the model are set to 0, running in text-only mode.
WARNING 11-22 17:04:21 [tokenizer.py:253] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 11-22 17:04:21 [registry.py:117] All limits of multimodal modalities supported by the model are set to 0, running in text-only mode.
WARNING 11-22 17:04:21 [tokenizer.py:253] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 11-22 17:04:21 [registry.py:117] All limits of multimodal modalities supported by the model are set to 0, running in text-only mode.
WARNING 11-22 17:04:21 [tokenizer.py:253] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 11-22 17:04:21 [registry.py:117] All limits of multimodal modalities supported by the model are set to 0, running in text-only mode.
WARNING 11-22 17:04:22 [tokenizer.py:253] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 11-22 17:04:22 [registry.py:117] All limits of multimodal modalities supported by the model are set to 0, running in text-only mode.
WARNING 11-22 17:04:22 [tokenizer.py:253] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 11-22 17:04:22 [registry.py:117] All limits of multimodal modalities supported by the model are set to 0, running in text-only mode.
WARNING 11-22 17:04:22 [tokenizer.py:253] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 11-22 17:04:22 [registry.py:117] All limits of multimodal modalities supported by the model are set to 0, running in text-only mode.
INFO 11-22 17:04:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_aeb67903'), local_subscribe_addr='ipc:///local/tmp/15bacc71-3745-46a3-95aa-0a42fbe8bda5', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 11-22 17:04:27 [tokenizer.py:253] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
INFO 11-22 17:04:27 [registry.py:117] All limits of multimodal modalities supported by the model are set to 0, running in text-only mode.
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
INFO 11-22 17:04:27 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:27 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:27 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:27 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:27 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:27 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:27 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:27 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:27 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:28 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:28 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:28 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:28 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:28 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:28 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:28 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 11-22 17:04:29 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
WARNING 11-22 17:04:29 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
WARNING 11-22 17:04:29 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
WARNING 11-22 17:04:29 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
WARNING 11-22 17:04:29 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
WARNING 11-22 17:04:29 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
WARNING 11-22 17:04:29 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
WARNING 11-22 17:04:29 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.
INFO 11-22 17:04:29 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
INFO 11-22 17:04:29 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
INFO 11-22 17:04:29 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
INFO 11-22 17:04:29 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
INFO 11-22 17:04:29 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
INFO 11-22 17:04:29 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
INFO 11-22 17:04:29 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
INFO 11-22 17:04:29 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
INFO 11-22 17:04:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_22797ec2'), local_subscribe_addr='ipc:///local/tmp/48e83b91-ca7c-4694-99e1-ff1bf66ecdcf', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
INFO 11-22 17:04:29 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:29 [__init__.py:1384] Found nccl from library libnccl.so.2
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
INFO 11-22 17:04:29 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:29 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:29 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:29 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:29 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:29 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:29 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:29 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:29 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:29 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:29 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:29 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 11-22 17:04:29 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:29 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 11-22 17:04:30 [parallel_state.py:1208] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4
INFO 11-22 17:04:30 [parallel_state.py:1208] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7
INFO 11-22 17:04:30 [parallel_state.py:1208] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5
INFO 11-22 17:04:30 [parallel_state.py:1208] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 11-22 17:04:30 [parallel_state.py:1208] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 11-22 17:04:30 [parallel_state.py:1208] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 11-22 17:04:30 [parallel_state.py:1208] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6
INFO 11-22 17:04:30 [parallel_state.py:1208] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
WARNING 11-22 17:04:31 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 11-22 17:04:31 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 11-22 17:04:31 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 11-22 17:04:31 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 11-22 17:04:31 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 11-22 17:04:31 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 11-22 17:04:31 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 11-22 17:04:31 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP4 pid=3810162)[0;0m INFO 11-22 17:04:31 [gpu_model_runner.py:2602] Starting to load model internlm/Intern-S1...
[1;36m(Worker_TP2 pid=3810160)[0;0m INFO 11-22 17:04:31 [gpu_model_runner.py:2602] Starting to load model internlm/Intern-S1...
[1;36m(Worker_TP5 pid=3810163)[0;0m INFO 11-22 17:04:31 [gpu_model_runner.py:2602] Starting to load model internlm/Intern-S1...
[1;36m(Worker_TP3 pid=3810161)[0;0m INFO 11-22 17:04:31 [gpu_model_runner.py:2602] Starting to load model internlm/Intern-S1...
[1;36m(Worker_TP7 pid=3810165)[0;0m INFO 11-22 17:04:31 [gpu_model_runner.py:2602] Starting to load model internlm/Intern-S1...
[1;36m(Worker_TP6 pid=3810164)[0;0m INFO 11-22 17:04:31 [gpu_model_runner.py:2602] Starting to load model internlm/Intern-S1...
[1;36m(Worker_TP0 pid=3810158)[0;0m INFO 11-22 17:04:31 [gpu_model_runner.py:2602] Starting to load model internlm/Intern-S1...
[1;36m(Worker_TP1 pid=3810159)[0;0m INFO 11-22 17:04:31 [gpu_model_runner.py:2602] Starting to load model internlm/Intern-S1...
[1;36m(Worker_TP4 pid=3810162)[0;0m INFO 11-22 17:04:31 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP4 pid=3810162)[0;0m INFO 11-22 17:04:31 [layer.py:444] MultiHeadAttention attn_backend: _Backend.FLASH_ATTN, use_upstream_fa: True
[1;36m(Worker_TP6 pid=3810164)[0;0m INFO 11-22 17:04:31 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=3810161)[0;0m INFO 11-22 17:04:31 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP6 pid=3810164)[0;0m INFO 11-22 17:04:31 [layer.py:444] MultiHeadAttention attn_backend: _Backend.FLASH_ATTN, use_upstream_fa: True
[1;36m(Worker_TP3 pid=3810161)[0;0m INFO 11-22 17:04:31 [layer.py:444] MultiHeadAttention attn_backend: _Backend.FLASH_ATTN, use_upstream_fa: True
[1;36m(Worker_TP7 pid=3810165)[0;0m INFO 11-22 17:04:32 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=3810160)[0;0m INFO 11-22 17:04:32 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP7 pid=3810165)[0;0m INFO 11-22 17:04:32 [layer.py:444] MultiHeadAttention attn_backend: _Backend.FLASH_ATTN, use_upstream_fa: True
[1;36m(Worker_TP0 pid=3810158)[0;0m INFO 11-22 17:04:32 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=3810160)[0;0m INFO 11-22 17:04:32 [layer.py:444] MultiHeadAttention attn_backend: _Backend.FLASH_ATTN, use_upstream_fa: True
[1;36m(Worker_TP5 pid=3810163)[0;0m INFO 11-22 17:04:32 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=3810158)[0;0m INFO 11-22 17:04:32 [layer.py:444] MultiHeadAttention attn_backend: _Backend.FLASH_ATTN, use_upstream_fa: True
[1;36m(Worker_TP1 pid=3810159)[0;0m INFO 11-22 17:04:32 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP5 pid=3810163)[0;0m INFO 11-22 17:04:32 [layer.py:444] MultiHeadAttention attn_backend: _Backend.FLASH_ATTN, use_upstream_fa: True
[1;36m(Worker_TP1 pid=3810159)[0;0m INFO 11-22 17:04:32 [layer.py:444] MultiHeadAttention attn_backend: _Backend.FLASH_ATTN, use_upstream_fa: True
[1;36m(Worker_TP4 pid=3810162)[0;0m INFO 11-22 17:04:32 [__init__.py:381] Cudagraph is disabled under eager mode
[1;36m(Worker_TP3 pid=3810161)[0;0m INFO 11-22 17:04:32 [__init__.py:381] Cudagraph is disabled under eager mode
[1;36m(Worker_TP6 pid=3810164)[0;0m INFO 11-22 17:04:32 [__init__.py:381] Cudagraph is disabled under eager mode
[1;36m(Worker_TP2 pid=3810160)[0;0m INFO 11-22 17:04:32 [__init__.py:381] Cudagraph is disabled under eager mode
[1;36m(Worker_TP7 pid=3810165)[0;0m INFO 11-22 17:04:32 [__init__.py:381] Cudagraph is disabled under eager mode
[1;36m(Worker_TP0 pid=3810158)[0;0m INFO 11-22 17:04:32 [__init__.py:381] Cudagraph is disabled under eager mode
[1;36m(Worker_TP1 pid=3810159)[0;0m INFO 11-22 17:04:32 [__init__.py:381] Cudagraph is disabled under eager mode
[1;36m(Worker_TP5 pid=3810163)[0;0m INFO 11-22 17:04:32 [__init__.py:381] Cudagraph is disabled under eager mode
[1;36m(Worker_TP3 pid=3810161)[0;0m INFO 11-22 17:04:32 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP4 pid=3810162)[0;0m INFO 11-22 17:04:32 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP6 pid=3810164)[0;0m INFO 11-22 17:04:32 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=3810158)[0;0m INFO 11-22 17:04:32 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP7 pid=3810165)[0;0m INFO 11-22 17:04:32 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=3810160)[0;0m INFO 11-22 17:04:32 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=3810159)[0;0m INFO 11-22 17:04:32 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP5 pid=3810163)[0;0m INFO 11-22 17:04:32 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP4 pid=3810162)[0;0m INFO 11-22 17:04:33 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP6 pid=3810164)[0;0m INFO 11-22 17:04:33 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=3810161)[0;0m INFO 11-22 17:04:33 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=3810160)[0;0m INFO 11-22 17:04:33 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP7 pid=3810165)[0;0m INFO 11-22 17:04:33 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP5 pid=3810163)[0;0m INFO 11-22 17:04:33 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=3810158)[0;0m INFO 11-22 17:04:33 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=3810159)[0;0m INFO 11-22 17:04:33 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/97 [00:00<?, ?it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:   1% Completed | 1/97 [00:00<00:34,  2.81it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:   2% Completed | 2/97 [00:00<00:49,  1.91it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:   3% Completed | 3/97 [00:01<00:59,  1.57it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:   4% Completed | 4/97 [00:02<00:56,  1.65it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:   5% Completed | 5/97 [00:02<00:52,  1.76it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:   6% Completed | 6/97 [00:03<00:49,  1.84it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:   7% Completed | 7/97 [00:03<00:46,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:   8% Completed | 8/97 [00:04<00:44,  2.00it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:   9% Completed | 9/97 [00:04<00:42,  2.06it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  10% Completed | 10/97 [00:05<00:42,  2.07it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  11% Completed | 11/97 [00:05<00:42,  2.04it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  12% Completed | 12/97 [00:06<00:41,  2.03it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  13% Completed | 13/97 [00:06<00:41,  2.02it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  14% Completed | 14/97 [00:07<00:41,  2.01it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  15% Completed | 15/97 [00:07<00:40,  2.01it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  16% Completed | 16/97 [00:08<00:40,  2.02it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  18% Completed | 17/97 [00:08<00:39,  2.02it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  19% Completed | 18/97 [00:09<00:38,  2.04it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  20% Completed | 19/97 [00:09<00:37,  2.08it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  21% Completed | 20/97 [00:10<00:37,  2.08it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  22% Completed | 21/97 [00:10<00:36,  2.07it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  23% Completed | 22/97 [00:11<00:36,  2.06it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  24% Completed | 23/97 [00:11<00:36,  2.04it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  25% Completed | 24/97 [00:12<00:36,  2.02it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  26% Completed | 25/97 [00:12<00:36,  2.00it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  27% Completed | 26/97 [00:13<00:35,  1.98it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  28% Completed | 27/97 [00:13<00:35,  1.97it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  29% Completed | 28/97 [00:14<00:34,  1.98it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  30% Completed | 29/97 [00:14<00:34,  1.99it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  31% Completed | 30/97 [00:15<00:33,  1.97it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  32% Completed | 31/97 [00:15<00:33,  1.95it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  33% Completed | 32/97 [00:16<00:33,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  34% Completed | 33/97 [00:16<00:33,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  35% Completed | 34/97 [00:17<00:32,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  36% Completed | 35/97 [00:17<00:32,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  37% Completed | 36/97 [00:18<00:31,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  38% Completed | 37/97 [00:18<00:33,  1.77it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  39% Completed | 38/97 [00:19<00:32,  1.81it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  40% Completed | 39/97 [00:19<00:31,  1.85it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  41% Completed | 40/97 [00:20<00:30,  1.87it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  42% Completed | 41/97 [00:21<00:29,  1.89it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  43% Completed | 42/97 [00:21<00:29,  1.89it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  44% Completed | 43/97 [00:22<00:28,  1.90it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  45% Completed | 44/97 [00:22<00:27,  1.92it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  46% Completed | 45/97 [00:23<00:27,  1.91it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  47% Completed | 46/97 [00:23<00:26,  1.91it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  48% Completed | 47/97 [00:24<00:26,  1.92it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  49% Completed | 48/97 [00:24<00:25,  1.92it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  51% Completed | 49/97 [00:25<00:24,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  52% Completed | 50/97 [00:25<00:24,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  53% Completed | 51/97 [00:26<00:23,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  54% Completed | 52/97 [00:26<00:23,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  55% Completed | 53/97 [00:27<00:22,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  56% Completed | 54/97 [00:27<00:22,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  57% Completed | 55/97 [00:28<00:21,  1.92it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  58% Completed | 56/97 [00:28<00:21,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  59% Completed | 57/97 [00:29<00:20,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  60% Completed | 58/97 [00:29<00:20,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  61% Completed | 59/97 [00:30<00:19,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  62% Completed | 60/97 [00:30<00:19,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  63% Completed | 61/97 [00:31<00:18,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  64% Completed | 62/97 [00:31<00:18,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  65% Completed | 63/97 [00:32<00:17,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  66% Completed | 64/97 [00:32<00:17,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  67% Completed | 65/97 [00:33<00:16,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  68% Completed | 66/97 [00:33<00:15,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  69% Completed | 67/97 [00:34<00:15,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  70% Completed | 68/97 [00:34<00:14,  1.95it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  71% Completed | 69/97 [00:35<00:14,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  72% Completed | 70/97 [00:36<00:13,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  73% Completed | 71/97 [00:36<00:13,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  74% Completed | 72/97 [00:37<00:12,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  75% Completed | 73/97 [00:37<00:12,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  76% Completed | 74/97 [00:38<00:11,  1.92it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  77% Completed | 75/97 [00:38<00:11,  1.92it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  78% Completed | 76/97 [00:39<00:10,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  79% Completed | 77/97 [00:39<00:10,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  80% Completed | 78/97 [00:40<00:09,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  81% Completed | 79/97 [00:40<00:09,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  82% Completed | 80/97 [00:41<00:08,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  84% Completed | 81/97 [00:41<00:08,  1.93it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  85% Completed | 82/97 [00:42<00:07,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  86% Completed | 83/97 [00:42<00:07,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  87% Completed | 84/97 [00:43<00:06,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  88% Completed | 85/97 [00:43<00:06,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  89% Completed | 86/97 [00:44<00:05,  1.94it/s]
[1;36m(Worker_TP3 pid=3810161)[0;0m INFO 11-22 17:05:19 [default_loader.py:267] Loading weights took 44.17 seconds
[1;36m(Worker_TP7 pid=3810165)[0;0m INFO 11-22 17:05:19 [default_loader.py:267] Loading weights took 44.03 seconds
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  90% Completed | 87/97 [00:44<00:05,  1.94it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  91% Completed | 88/97 [00:45<00:04,  1.94it/s]
[1;36m(Worker_TP7 pid=3810165)[0;0m INFO 11-22 17:05:20 [gpu_model_runner.py:2653] Model loading took 59.4657 GiB and 47.311087 seconds
[1;36m(Worker_TP6 pid=3810164)[0;0m INFO 11-22 17:05:20 [default_loader.py:267] Loading weights took 44.65 seconds
[1;36m(Worker_TP3 pid=3810161)[0;0m INFO 11-22 17:05:20 [gpu_model_runner.py:2653] Model loading took 59.4657 GiB and 47.249484 seconds
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  92% Completed | 89/97 [00:45<00:04,  1.94it/s]
[1;36m(Worker_TP6 pid=3810164)[0;0m INFO 11-22 17:05:20 [gpu_model_runner.py:2653] Model loading took 59.4657 GiB and 48.099417 seconds
[1;36m(Worker_TP2 pid=3810160)[0;0m INFO 11-22 17:05:20 [default_loader.py:267] Loading weights took 45.29 seconds
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  93% Completed | 90/97 [00:46<00:03,  1.95it/s]
[1;36m(Worker_TP2 pid=3810160)[0;0m INFO 11-22 17:05:21 [gpu_model_runner.py:2653] Model loading took 59.4657 GiB and 48.826970 seconds
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  94% Completed | 91/97 [00:46<00:03,  1.96it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  95% Completed | 92/97 [00:47<00:02,  1.97it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  96% Completed | 93/97 [00:47<00:02,  1.96it/s]
[1;36m(Worker_TP5 pid=3810163)[0;0m INFO 11-22 17:05:22 [default_loader.py:267] Loading weights took 48.67 seconds
[1;36m(Worker_TP4 pid=3810162)[0;0m INFO 11-22 17:05:22 [default_loader.py:267] Loading weights took 48.99 seconds
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  97% Completed | 94/97 [00:48<00:01,  1.95it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  98% Completed | 95/97 [00:48<00:00,  2.30it/s]
[1;36m(Worker_TP5 pid=3810163)[0;0m INFO 11-22 17:05:23 [gpu_model_runner.py:2653] Model loading took 59.4657 GiB and 50.759750 seconds
[1;36m(Worker_TP4 pid=3810162)[0;0m INFO 11-22 17:05:23 [gpu_model_runner.py:2653] Model loading took 59.4657 GiB and 50.897959 seconds
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards:  99% Completed | 96/97 [00:49<00:00,  2.30it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards: 100% Completed | 97/97 [00:49<00:00,  2.18it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m Loading safetensors checkpoint shards: 100% Completed | 97/97 [00:49<00:00,  1.96it/s]
[1;36m(Worker_TP0 pid=3810158)[0;0m 
[1;36m(Worker_TP0 pid=3810158)[0;0m INFO 11-22 17:05:24 [default_loader.py:267] Loading weights took 49.71 seconds
[1;36m(Worker_TP0 pid=3810158)[0;0m INFO 11-22 17:05:25 [gpu_model_runner.py:2653] Model loading took 59.4657 GiB and 52.374727 seconds
[1;36m(Worker_TP1 pid=3810159)[0;0m INFO 11-22 17:05:26 [default_loader.py:267] Loading weights took 50.80 seconds
[1;36m(Worker_TP1 pid=3810159)[0;0m INFO 11-22 17:05:27 [gpu_model_runner.py:2653] Model loading took 59.4657 GiB and 54.578172 seconds
[1;36m(Worker_TP1 pid=3810159)[0;0m INFO 11-22 17:05:32 [fused_moe.py:788] Using configuration from /data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100-SXM4-80GB.json for MoE layer.
[1;36m(Worker_TP6 pid=3810164)[0;0m INFO 11-22 17:05:32 [fused_moe.py:788] Using configuration from /data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100-SXM4-80GB.json for MoE layer.
[1;36m(Worker_TP4 pid=3810162)[0;0m INFO 11-22 17:05:32 [fused_moe.py:788] Using configuration from /data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100-SXM4-80GB.json for MoE layer.
[1;36m(Worker_TP5 pid=3810163)[0;0m INFO 11-22 17:05:32 [fused_moe.py:788] Using configuration from /data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100-SXM4-80GB.json for MoE layer.
[1;36m(Worker_TP2 pid=3810160)[0;0m INFO 11-22 17:05:32 [fused_moe.py:788] Using configuration from /data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100-SXM4-80GB.json for MoE layer.
[1;36m(Worker_TP0 pid=3810158)[0;0m INFO 11-22 17:05:32 [fused_moe.py:788] Using configuration from /data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100-SXM4-80GB.json for MoE layer.
[1;36m(Worker_TP3 pid=3810161)[0;0m INFO 11-22 17:05:32 [fused_moe.py:788] Using configuration from /data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100-SXM4-80GB.json for MoE layer.
[1;36m(Worker_TP7 pid=3810165)[0;0m INFO 11-22 17:05:32 [fused_moe.py:788] Using configuration from /data1/tianang/anaconda3/envs/intern/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100-SXM4-80GB.json for MoE layer.
[1;36m(Worker_TP5 pid=3810163)[0;0m INFO 11-22 17:05:35 [gpu_worker.py:298] Available KV cache memory: 10.79 GiB
[1;36m(Worker_TP4 pid=3810162)[0;0m INFO 11-22 17:05:35 [gpu_worker.py:298] Available KV cache memory: 10.79 GiB
[1;36m(Worker_TP0 pid=3810158)[0;0m INFO 11-22 17:05:35 [gpu_worker.py:298] Available KV cache memory: 10.79 GiB
[1;36m(Worker_TP2 pid=3810160)[0;0m INFO 11-22 17:05:35 [gpu_worker.py:298] Available KV cache memory: 10.79 GiB
[1;36m(Worker_TP6 pid=3810164)[0;0m INFO 11-22 17:05:35 [gpu_worker.py:298] Available KV cache memory: 10.79 GiB
[1;36m(Worker_TP1 pid=3810159)[0;0m INFO 11-22 17:05:35 [gpu_worker.py:298] Available KV cache memory: 10.79 GiB
[1;36m(Worker_TP3 pid=3810161)[0;0m INFO 11-22 17:05:35 [gpu_worker.py:298] Available KV cache memory: 10.79 GiB
[1;36m(Worker_TP7 pid=3810165)[0;0m INFO 11-22 17:05:36 [gpu_worker.py:298] Available KV cache memory: 10.79 GiB
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1087] GPU KV cache size: 240,640 tokens
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1091] Maximum concurrency for 24,576 tokens per request: 9.79x
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1087] GPU KV cache size: 240,640 tokens
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1091] Maximum concurrency for 24,576 tokens per request: 9.79x
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1087] GPU KV cache size: 240,640 tokens
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1091] Maximum concurrency for 24,576 tokens per request: 9.79x
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1087] GPU KV cache size: 240,640 tokens
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1091] Maximum concurrency for 24,576 tokens per request: 9.79x
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1087] GPU KV cache size: 240,640 tokens
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1091] Maximum concurrency for 24,576 tokens per request: 9.79x
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1087] GPU KV cache size: 240,640 tokens
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1091] Maximum concurrency for 24,576 tokens per request: 9.79x
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1087] GPU KV cache size: 240,640 tokens
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1091] Maximum concurrency for 24,576 tokens per request: 9.79x
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1087] GPU KV cache size: 240,640 tokens
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:36 [kv_cache_utils.py:1091] Maximum concurrency for 24,576 tokens per request: 9.79x
[1;36m(Worker_TP5 pid=3810163)[0;0m WARNING 11-22 17:05:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP7 pid=3810165)[0;0m WARNING 11-22 17:05:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP4 pid=3810162)[0;0m WARNING 11-22 17:05:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=3810160)[0;0m WARNING 11-22 17:05:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP3 pid=3810161)[0;0m WARNING 11-22 17:05:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP0 pid=3810158)[0;0m WARNING 11-22 17:05:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=3810159)[0;0m WARNING 11-22 17:05:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP6 pid=3810164)[0;0m WARNING 11-22 17:05:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:37 [core.py:210] init engine (profile, create kv cache, warmup model) took 9.97 seconds
[1;36m(EngineCore_DP0 pid=3809986)[0;0m WARNING 11-22 17:05:38 [tokenizer.py:253] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
[1;36m(EngineCore_DP0 pid=3809986)[0;0m WARNING 11-22 17:05:41 [tokenizer.py:253] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:41 [registry.py:117] All limits of multimodal modalities supported by the model are set to 0, running in text-only mode.
[1;36m(EngineCore_DP0 pid=3809986)[0;0m INFO 11-22 17:05:41 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 11-22 17:05:41 [llm.py:306] Supported_tasks: ['generate']
[PPI] Processing tasks ...:   0%|          | 0/1 [00:00<?, ?it/s]Found local copy...
Loading...
Done!
2025-11-22 17:05:44,544 - INFO - [PPI/HuRI] Total test samples: 20948

Adding requests:   0%|          | 0/1 [00:00<?, ?it/s][AAdding requests: 100%|██████████| 1/1 [00:00<00:00, 57.46it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A